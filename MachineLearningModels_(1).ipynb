{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAux-cx8dlbr"
      },
      "source": [
        "# Predicting Song Popularity using Machine Learning\n",
        "\n",
        "This Jupyter Notebook uses several machine learning algorithms to predict the popularity of a song. The dataset used in this notebook is a cleaned and preprocessed version of the original dataset containing the audio features of songs. We define the top 25% popular songs as \"popular\", and the bottom 75% popular songs as \"not popular\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WsVlOJP6dlbs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd # for working with songDatas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "X6pqHKVCdlbt",
        "outputId": "65e33f6e-5481-4bad-d15b-50913a07baff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                               name  \\\n",
              "0           0                           Keep A Song In Your Soul   \n",
              "1           1                               I Put A Spell On You   \n",
              "2           2                                       Golfing Papa   \n",
              "3           3  True House Music - Xavier Santos & Carlos Gomi...   \n",
              "4           4                                          Xuniverxe   \n",
              "\n",
              "                     artists  popularity release_date  acousticness  \\\n",
              "0            ['Mamie Smith']          12   1920-01-01      0.991000   \n",
              "1  [\"Screamin' Jay Hawkins\"]           7   1920-05-01      0.643000   \n",
              "2            ['Mamie Smith']           4   1920-01-01      0.993000   \n",
              "3        ['Oscar Velazquez']          17   1920-01-01      0.000173   \n",
              "4                   ['Mixe']           2   1920-01-10      0.295000   \n",
              "\n",
              "   danceability  duration_ms  energy  explicit  instrumentalness  key  \\\n",
              "0         0.598       168333   0.224         0          0.000522    5   \n",
              "1         0.852       150200   0.517         0          0.026400    5   \n",
              "2         0.647       163827   0.186         0          0.000018    0   \n",
              "3         0.730       422087   0.798         0          0.801000    2   \n",
              "4         0.704       165224   0.707         1          0.000246   10   \n",
              "\n",
              "   liveness  loudness  mode  speechiness    tempo  valence  \n",
              "0    0.3790   -12.628     0       0.0936  149.976   0.6340  \n",
              "1    0.0809    -7.261     0       0.0534   86.889   0.9500  \n",
              "2    0.5190   -12.098     1       0.1740   97.600   0.6890  \n",
              "3    0.1280    -7.311     1       0.0425  127.997   0.0422  \n",
              "4    0.4020    -6.036     0       0.0768  122.076   0.2990  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85669b5d-e7af-4664-83bd-fc802c48ebb8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>name</th>\n",
              "      <th>artists</th>\n",
              "      <th>popularity</th>\n",
              "      <th>release_date</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>explicit</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Keep A Song In Your Soul</td>\n",
              "      <td>['Mamie Smith']</td>\n",
              "      <td>12</td>\n",
              "      <td>1920-01-01</td>\n",
              "      <td>0.991000</td>\n",
              "      <td>0.598</td>\n",
              "      <td>168333</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>5</td>\n",
              "      <td>0.3790</td>\n",
              "      <td>-12.628</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0936</td>\n",
              "      <td>149.976</td>\n",
              "      <td>0.6340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I Put A Spell On You</td>\n",
              "      <td>[\"Screamin' Jay Hawkins\"]</td>\n",
              "      <td>7</td>\n",
              "      <td>1920-05-01</td>\n",
              "      <td>0.643000</td>\n",
              "      <td>0.852</td>\n",
              "      <td>150200</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0</td>\n",
              "      <td>0.026400</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0809</td>\n",
              "      <td>-7.261</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0534</td>\n",
              "      <td>86.889</td>\n",
              "      <td>0.9500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Golfing Papa</td>\n",
              "      <td>['Mamie Smith']</td>\n",
              "      <td>4</td>\n",
              "      <td>1920-01-01</td>\n",
              "      <td>0.993000</td>\n",
              "      <td>0.647</td>\n",
              "      <td>163827</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5190</td>\n",
              "      <td>-12.098</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1740</td>\n",
              "      <td>97.600</td>\n",
              "      <td>0.6890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>True House Music - Xavier Santos &amp; Carlos Gomi...</td>\n",
              "      <td>['Oscar Velazquez']</td>\n",
              "      <td>17</td>\n",
              "      <td>1920-01-01</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.730</td>\n",
              "      <td>422087</td>\n",
              "      <td>0.798</td>\n",
              "      <td>0</td>\n",
              "      <td>0.801000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1280</td>\n",
              "      <td>-7.311</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0425</td>\n",
              "      <td>127.997</td>\n",
              "      <td>0.0422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Xuniverxe</td>\n",
              "      <td>['Mixe']</td>\n",
              "      <td>2</td>\n",
              "      <td>1920-01-10</td>\n",
              "      <td>0.295000</td>\n",
              "      <td>0.704</td>\n",
              "      <td>165224</td>\n",
              "      <td>0.707</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>10</td>\n",
              "      <td>0.4020</td>\n",
              "      <td>-6.036</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0768</td>\n",
              "      <td>122.076</td>\n",
              "      <td>0.2990</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85669b5d-e7af-4664-83bd-fc802c48ebb8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-85669b5d-e7af-4664-83bd-fc802c48ebb8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-85669b5d-e7af-4664-83bd-fc802c48ebb8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-29bedd9d-8330-4ace-838c-e9587e080c96\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-29bedd9d-8330-4ace-838c-e9587e080c96')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-29bedd9d-8330-4ace-838c-e9587e080c96 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "songData"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "songData = pd.read_csv('cleaned-song-dataset.csv')\n",
        "songData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "OPkUvTXIdlbu",
        "outputId": "123eaf2f-eff0-4d89-f148-1e10e85e0db7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Unnamed: 0     popularity   acousticness   danceability  \\\n",
              "count  133484.000000  133484.000000  133484.000000  133484.000000   \n",
              "mean    86738.448623      33.566892       0.445756       0.537559   \n",
              "std     50840.922522      18.992977       0.360302       0.173297   \n",
              "min         0.000000       1.000000       0.000000       0.000000   \n",
              "25%     43371.250000      20.000000       0.070700       0.421000   \n",
              "50%     86632.500000      33.000000       0.412000       0.547000   \n",
              "75%    132120.250000      47.000000       0.805000       0.663000   \n",
              "max    174387.000000     100.000000       0.996000       0.988000   \n",
              "\n",
              "        duration_ms         energy       explicit  instrumentalness  \\\n",
              "count  1.334840e+05  133484.000000  133484.000000     133484.000000   \n",
              "mean   2.328956e+05       0.517069       0.064457          0.152212   \n",
              "std    1.273368e+05       0.266594       0.245566          0.301002   \n",
              "min    1.470800e+04       0.000000       0.000000          0.000000   \n",
              "25%    1.696658e+05       0.299000       0.000000          0.000000   \n",
              "50%    2.124000e+05       0.519000       0.000000          0.000179   \n",
              "75%    2.679730e+05       0.737000       0.000000          0.061800   \n",
              "max    4.892761e+06       1.000000       1.000000          1.000000   \n",
              "\n",
              "                 key       liveness       loudness           mode  \\\n",
              "count  133484.000000  133484.000000  133484.000000  133484.000000   \n",
              "mean        5.198271       0.208644     -11.092275       0.711703   \n",
              "std         3.510869       0.183613       5.358354       0.452972   \n",
              "min         0.000000       0.000000     -60.000000       0.000000   \n",
              "25%         2.000000       0.096400     -13.922000       0.000000   \n",
              "50%         5.000000       0.134000     -10.266500       1.000000   \n",
              "75%         8.000000       0.265000      -7.144000       1.000000   \n",
              "max        11.000000       1.000000       3.744000       1.000000   \n",
              "\n",
              "         speechiness          tempo        valence  \n",
              "count  133484.000000  133484.000000  133484.000000  \n",
              "mean        0.079103     118.397793       0.533009  \n",
              "std         0.118517      30.009354       0.263969  \n",
              "min         0.000000       0.000000       0.000000  \n",
              "25%         0.033900      95.357750       0.319000  \n",
              "50%         0.042900     116.463500       0.543000  \n",
              "75%         0.066800     136.567000       0.754000  \n",
              "max         0.971000     243.507000       1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56169bb6-d0fd-452a-a264-190a85029bbe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>popularity</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>explicit</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>1.334840e+05</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "      <td>133484.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>86738.448623</td>\n",
              "      <td>33.566892</td>\n",
              "      <td>0.445756</td>\n",
              "      <td>0.537559</td>\n",
              "      <td>2.328956e+05</td>\n",
              "      <td>0.517069</td>\n",
              "      <td>0.064457</td>\n",
              "      <td>0.152212</td>\n",
              "      <td>5.198271</td>\n",
              "      <td>0.208644</td>\n",
              "      <td>-11.092275</td>\n",
              "      <td>0.711703</td>\n",
              "      <td>0.079103</td>\n",
              "      <td>118.397793</td>\n",
              "      <td>0.533009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>50840.922522</td>\n",
              "      <td>18.992977</td>\n",
              "      <td>0.360302</td>\n",
              "      <td>0.173297</td>\n",
              "      <td>1.273368e+05</td>\n",
              "      <td>0.266594</td>\n",
              "      <td>0.245566</td>\n",
              "      <td>0.301002</td>\n",
              "      <td>3.510869</td>\n",
              "      <td>0.183613</td>\n",
              "      <td>5.358354</td>\n",
              "      <td>0.452972</td>\n",
              "      <td>0.118517</td>\n",
              "      <td>30.009354</td>\n",
              "      <td>0.263969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.470800e+04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>43371.250000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.070700</td>\n",
              "      <td>0.421000</td>\n",
              "      <td>1.696658e+05</td>\n",
              "      <td>0.299000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.096400</td>\n",
              "      <td>-13.922000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033900</td>\n",
              "      <td>95.357750</td>\n",
              "      <td>0.319000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>86632.500000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>0.412000</td>\n",
              "      <td>0.547000</td>\n",
              "      <td>2.124000e+05</td>\n",
              "      <td>0.519000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.134000</td>\n",
              "      <td>-10.266500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.042900</td>\n",
              "      <td>116.463500</td>\n",
              "      <td>0.543000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>132120.250000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>0.805000</td>\n",
              "      <td>0.663000</td>\n",
              "      <td>2.679730e+05</td>\n",
              "      <td>0.737000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.061800</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>-7.144000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.066800</td>\n",
              "      <td>136.567000</td>\n",
              "      <td>0.754000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>174387.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.996000</td>\n",
              "      <td>0.988000</td>\n",
              "      <td>4.892761e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.744000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.971000</td>\n",
              "      <td>243.507000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56169bb6-d0fd-452a-a264-190a85029bbe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-56169bb6-d0fd-452a-a264-190a85029bbe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-56169bb6-d0fd-452a-a264-190a85029bbe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4c13b1a7-5ce5-4bb9-898b-d92cac96cbb5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c13b1a7-5ce5-4bb9-898b-d92cac96cbb5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4c13b1a7-5ce5-4bb9-898b-d92cac96cbb5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"songData\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56816.10863457554,\n        \"min\": 0.0,\n        \"max\": 174387.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          86738.44862305594,\n          86632.5,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"popularity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47180.92316891832,\n        \"min\": 1.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          33.56689191213928,\n          33.0,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"acousticness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47193.564734803054,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.4457556546932217,\n          0.412,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"danceability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47193.552607773694,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5375586085223696,\n          0.547,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1673155.4945359114,\n        \"min\": 14708.0,\n        \"max\": 4892761.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          232895.55644871294,\n          212400.0,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"energy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47193.552163004155,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5170689541578016,\n          0.519,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"explicit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47193.65462500744,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.06445716340535196,\n          1.0,\n          0.24556646605810611\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"instrumentalness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47193.64426236447,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          133484.0,\n          0.15221180564891673,\n          0.0618\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"key\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47191.96783660663,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.198270953822181,\n          5.0,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"liveness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47193.62544988693,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.20864368366246144,\n          0.134,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loudness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47198.438630210556,\n        \"min\": -60.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          -11.092275209013813,\n          -10.2665,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mode\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47193.510443974556,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7117032752989122,\n          1.0,\n          0.45297158891612654\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speechiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47193.654513923655,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.07910328728536753,\n          0.0429,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tempo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47156.38626409007,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          118.39779276917083,\n          116.4635,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47193.54840952363,\n        \"min\": 0.0,\n        \"max\": 133484.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5330088648077672,\n          0.543,\n          133484.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "songData.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b446p5mdlbu"
      },
      "source": [
        "## Data Preparation\n",
        "First, we load the preprocessed dataset using pandas and explore it using the head() and describe() methods. We then preprocess the dataset by converting the popularity score to a binary classification problem using a threshold of 47 for popularity (75th percentile).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "njA412Wzdlbu",
        "outputId": "3149a1d3-3d76-4c52-fbce-eaff9c09e5a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0                                               name  \\\n",
              "312           1062                                  Ain't Misbehavin'   \n",
              "524           1462                                   Sing, Sing, Sing   \n",
              "663           1662                                     Mack the Knife   \n",
              "689           1862  Hungarian Rhapsody No. 2 in C-Sharp Minor, S. ...   \n",
              "952           2462     All of Me (with Eddie Heywood & His Orchestra)   \n",
              "...            ...                                                ...   \n",
              "133475      174351                                   Waiting On A War   \n",
              "133476      174353                                     Precious' Tale   \n",
              "133477      174355                                          Connexion   \n",
              "133479      174361                                         Little Boy   \n",
              "133483      174387                                 champagne problems   \n",
              "\n",
              "                                     artists  popularity release_date  \\\n",
              "312                          ['Fats Waller']           1   1926-01-01   \n",
              "524                        ['Benny Goodman']           1   1928-01-01   \n",
              "663                      ['Louis Armstrong']           1   1929-01-01   \n",
              "689     ['Franz Liszt', 'Vladimir Horowitz']           1   1930-01-01   \n",
              "952      ['Billie Holiday', 'Eddie Heywood']           1   1933-01-01   \n",
              "...                                      ...         ...          ...   \n",
              "133475                      ['Foo Fighters']           1   2021-01-14   \n",
              "133476                  ['Jazmine Sullivan']           1   2021-08-01   \n",
              "133477                              ['ZAYN']           1   2021-01-15   \n",
              "133479                          ['Ashnikko']           1   2021-01-15   \n",
              "133483                      ['Taylor Swift']           1   2021-07-01   \n",
              "\n",
              "        acousticness  danceability  duration_ms  energy  explicit  \\\n",
              "312          0.82100         0.515       237773  0.2220         0   \n",
              "524          0.84700         0.626       520133  0.7440         0   \n",
              "663          0.58600         0.673       201467  0.3770         0   \n",
              "689          0.98700         0.349       541600  0.3260         0   \n",
              "952          0.97200         0.504       181440  0.0644         0   \n",
              "...              ...           ...          ...     ...       ...   \n",
              "133475       0.00984         0.530       253840  0.7590         0   \n",
              "133476       0.71500         0.734        43320  0.3460         0   \n",
              "133477       0.49800         0.597       196493  0.3680         0   \n",
              "133479       0.10500         0.781       172720  0.4870         1   \n",
              "133483       0.92000         0.462       244000  0.2400         1   \n",
              "\n",
              "        instrumentalness  key  liveness  loudness  mode  speechiness    tempo  \\\n",
              "312             0.001930    0    0.1900   -16.918     0       0.0575   98.358   \n",
              "524             0.892000    2    0.1450    -9.189     0       0.0662  113.117   \n",
              "663             0.000000    0    0.3320   -14.141     1       0.0697   88.973   \n",
              "689             0.886000    1    0.7840   -15.347     1       0.0551   80.233   \n",
              "952             0.000004    2    0.1740   -14.754     0       0.0408  106.994   \n",
              "...                  ...  ...       ...       ...   ...          ...      ...   \n",
              "133475          0.000000    7    0.3190    -7.067     1       0.0351  131.999   \n",
              "133476          0.000000    2    0.3940   -11.722     1       0.3550   88.849   \n",
              "133477          0.000000    2    0.1090   -10.151     0       0.0936  171.980   \n",
              "133479          0.000000    1    0.0802    -7.301     0       0.1670  129.941   \n",
              "133483          0.000000    0    0.1130   -12.077     1       0.0377  171.319   \n",
              "\n",
              "        valence  \n",
              "312       0.350  \n",
              "524       0.259  \n",
              "663       0.713  \n",
              "689       0.168  \n",
              "952       0.403  \n",
              "...         ...  \n",
              "133475    0.502  \n",
              "133476    0.930  \n",
              "133477    0.590  \n",
              "133479    0.327  \n",
              "133483    0.320  \n",
              "\n",
              "[34465 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43a73a97-1d2e-44c1-8495-d6bde5e3f58f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>name</th>\n",
              "      <th>artists</th>\n",
              "      <th>popularity</th>\n",
              "      <th>release_date</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>explicit</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>1062</td>\n",
              "      <td>Ain't Misbehavin'</td>\n",
              "      <td>['Fats Waller']</td>\n",
              "      <td>1</td>\n",
              "      <td>1926-01-01</td>\n",
              "      <td>0.82100</td>\n",
              "      <td>0.515</td>\n",
              "      <td>237773</td>\n",
              "      <td>0.2220</td>\n",
              "      <td>0</td>\n",
              "      <td>0.001930</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1900</td>\n",
              "      <td>-16.918</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0575</td>\n",
              "      <td>98.358</td>\n",
              "      <td>0.350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>1462</td>\n",
              "      <td>Sing, Sing, Sing</td>\n",
              "      <td>['Benny Goodman']</td>\n",
              "      <td>1</td>\n",
              "      <td>1928-01-01</td>\n",
              "      <td>0.84700</td>\n",
              "      <td>0.626</td>\n",
              "      <td>520133</td>\n",
              "      <td>0.7440</td>\n",
              "      <td>0</td>\n",
              "      <td>0.892000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1450</td>\n",
              "      <td>-9.189</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0662</td>\n",
              "      <td>113.117</td>\n",
              "      <td>0.259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>1662</td>\n",
              "      <td>Mack the Knife</td>\n",
              "      <td>['Louis Armstrong']</td>\n",
              "      <td>1</td>\n",
              "      <td>1929-01-01</td>\n",
              "      <td>0.58600</td>\n",
              "      <td>0.673</td>\n",
              "      <td>201467</td>\n",
              "      <td>0.3770</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.3320</td>\n",
              "      <td>-14.141</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0697</td>\n",
              "      <td>88.973</td>\n",
              "      <td>0.713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>689</th>\n",
              "      <td>1862</td>\n",
              "      <td>Hungarian Rhapsody No. 2 in C-Sharp Minor, S. ...</td>\n",
              "      <td>['Franz Liszt', 'Vladimir Horowitz']</td>\n",
              "      <td>1</td>\n",
              "      <td>1930-01-01</td>\n",
              "      <td>0.98700</td>\n",
              "      <td>0.349</td>\n",
              "      <td>541600</td>\n",
              "      <td>0.3260</td>\n",
              "      <td>0</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.7840</td>\n",
              "      <td>-15.347</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0551</td>\n",
              "      <td>80.233</td>\n",
              "      <td>0.168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>2462</td>\n",
              "      <td>All of Me (with Eddie Heywood &amp; His Orchestra)</td>\n",
              "      <td>['Billie Holiday', 'Eddie Heywood']</td>\n",
              "      <td>1</td>\n",
              "      <td>1933-01-01</td>\n",
              "      <td>0.97200</td>\n",
              "      <td>0.504</td>\n",
              "      <td>181440</td>\n",
              "      <td>0.0644</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1740</td>\n",
              "      <td>-14.754</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0408</td>\n",
              "      <td>106.994</td>\n",
              "      <td>0.403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133475</th>\n",
              "      <td>174351</td>\n",
              "      <td>Waiting On A War</td>\n",
              "      <td>['Foo Fighters']</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-01-14</td>\n",
              "      <td>0.00984</td>\n",
              "      <td>0.530</td>\n",
              "      <td>253840</td>\n",
              "      <td>0.7590</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7</td>\n",
              "      <td>0.3190</td>\n",
              "      <td>-7.067</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0351</td>\n",
              "      <td>131.999</td>\n",
              "      <td>0.502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133476</th>\n",
              "      <td>174353</td>\n",
              "      <td>Precious' Tale</td>\n",
              "      <td>['Jazmine Sullivan']</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-08-01</td>\n",
              "      <td>0.71500</td>\n",
              "      <td>0.734</td>\n",
              "      <td>43320</td>\n",
              "      <td>0.3460</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3940</td>\n",
              "      <td>-11.722</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3550</td>\n",
              "      <td>88.849</td>\n",
              "      <td>0.930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133477</th>\n",
              "      <td>174355</td>\n",
              "      <td>Connexion</td>\n",
              "      <td>['ZAYN']</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-01-15</td>\n",
              "      <td>0.49800</td>\n",
              "      <td>0.597</td>\n",
              "      <td>196493</td>\n",
              "      <td>0.3680</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1090</td>\n",
              "      <td>-10.151</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0936</td>\n",
              "      <td>171.980</td>\n",
              "      <td>0.590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133479</th>\n",
              "      <td>174361</td>\n",
              "      <td>Little Boy</td>\n",
              "      <td>['Ashnikko']</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-01-15</td>\n",
              "      <td>0.10500</td>\n",
              "      <td>0.781</td>\n",
              "      <td>172720</td>\n",
              "      <td>0.4870</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0802</td>\n",
              "      <td>-7.301</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1670</td>\n",
              "      <td>129.941</td>\n",
              "      <td>0.327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133483</th>\n",
              "      <td>174387</td>\n",
              "      <td>champagne problems</td>\n",
              "      <td>['Taylor Swift']</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-07-01</td>\n",
              "      <td>0.92000</td>\n",
              "      <td>0.462</td>\n",
              "      <td>244000</td>\n",
              "      <td>0.2400</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1130</td>\n",
              "      <td>-12.077</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0377</td>\n",
              "      <td>171.319</td>\n",
              "      <td>0.320</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>34465 rows  18 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43a73a97-1d2e-44c1-8495-d6bde5e3f58f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-43a73a97-1d2e-44c1-8495-d6bde5e3f58f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-43a73a97-1d2e-44c1-8495-d6bde5e3f58f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-104f2458-9195-452b-8102-b119601da860\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-104f2458-9195-452b-8102-b119601da860')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-104f2458-9195-452b-8102-b119601da860 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "songData.loc[songData['popularity'] < 47, 'popularity'] = 0\n",
        "songData.loc[songData['popularity'] >= 47, 'popularity'] = 1\n",
        "songData.loc[songData['popularity'] == 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udmf7-T5dlbv"
      },
      "source": [
        "## Model Training and Evaluation\n",
        "We use the following machine learning algorithms to predict the popularity of a song:\n",
        "\n",
        "**Logistic Regression**\n",
        "\n",
        "**Random Forest Classifier**\n",
        "\n",
        "**K-Nearest Neighbors Classifier**\n",
        "\n",
        "**Decision Tree Classifier**\n",
        "\n",
        "**Linear Support Vector Classification**\n",
        "\n",
        "**XGBoost**\n",
        "\n",
        "**LightGBM + Hyperparameter Tuning**\n",
        "\n",
        "**Voting Ensemble - LGBM, XGB, MLP**\n",
        "\n",
        "**Deep Learning - Neural Networks**\n",
        "\n",
        "**Deeper Neural Network**\n",
        "\n",
        "We use the training set to train a model for each algorithm, and the validation set is used to assess the model's performance. For evaluation, we employ the metrics roc_auc_score and accuracy_score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "h_rFYGpGdlbv"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EC9FTusAdlbv"
      },
      "outputs": [],
      "source": [
        "features = [\"acousticness\", \"danceability\", \"duration_ms\", \"energy\", \"instrumentalness\", \"key\", \"liveness\",\n",
        "            \"mode\", \"speechiness\", \"tempo\", \"valence\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P3X2g1yfdlbw"
      },
      "outputs": [],
      "source": [
        "training = songData.sample(frac = 0.8)\n",
        "X_train = training[features]\n",
        "y_train = training['popularity']\n",
        "X_test = songData.drop(training.index)[features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "w04YbN_Udlbw"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGP5lcpIdlbw"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYAsDBe9dlbx",
        "outputId": "b38f22ea-ec48-4296-be15-258d51a469eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7421106845210226\n",
            "AUC: 0.5\n"
          ]
        }
      ],
      "source": [
        "LR_Model = LogisticRegression()\n",
        "LR_Model.fit(X_train, y_train)\n",
        "LR_Predict = LR_Model.predict(X_valid)\n",
        "LR_Accuracy = accuracy_score(y_valid, LR_Predict)\n",
        "print(\"Accuracy: \" + str(LR_Accuracy))\n",
        "\n",
        "LR_AUC = roc_auc_score(y_valid, LR_Predict)\n",
        "print(\"AUC: \" + str(LR_AUC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ah8NwQCdlbx"
      },
      "source": [
        "**Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pTRlIO3dlbx",
        "outputId": "c942ae8b-4a7f-4f1b-adb8-01526a1d438a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7875737428598183\n",
            "AUC: 0.6473758273025298\n"
          ]
        }
      ],
      "source": [
        "RFC_Model = RandomForestClassifier()\n",
        "RFC_Model.fit(X_train, y_train)\n",
        "RFC_Predict = RFC_Model.predict(X_valid)\n",
        "RFC_Accuracy = accuracy_score(y_valid, RFC_Predict)\n",
        "print(\"Accuracy: \" + str(RFC_Accuracy))\n",
        "\n",
        "RFC_AUC = roc_auc_score(y_valid, RFC_Predict)\n",
        "print(\"AUC: \" + str(RFC_AUC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNbP7JIAdlbx"
      },
      "source": [
        "**K-Nearest Neighbors Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evghut-udlbx",
        "outputId": "4ba18e68-32cd-406d-ac60-7bcdab53793a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6958984923681992\n",
            "AUC: 0.5307018411991505\n"
          ]
        }
      ],
      "source": [
        "KNN_Model = KNeighborsClassifier()\n",
        "KNN_Model.fit(X_train, y_train)\n",
        "KNN_Predict = KNN_Model.predict(X_valid)\n",
        "KNN_Accuracy = accuracy_score(y_valid, KNN_Predict)\n",
        "print(\"Accuracy: \" + str(KNN_Accuracy))\n",
        "\n",
        "KNN_AUC = roc_auc_score(y_valid, KNN_Predict)\n",
        "print(\"AUC: \" + str(KNN_AUC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rgqo8Qidlby"
      },
      "source": [
        "**Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZSpFMPpdlby",
        "outputId": "773d97d8-a37d-4728-90f6-41291847237a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6940724786965071\n",
            "AUC: 0.6095522887271511\n"
          ]
        }
      ],
      "source": [
        "DT_Model = DecisionTreeClassifier()\n",
        "DT_Model.fit(X_train, y_train)\n",
        "DT_Predict = DT_Model.predict(X_valid)\n",
        "DT_Accuracy = accuracy_score(y_valid, DT_Predict)\n",
        "print(\"Accuracy: \" + str(DT_Accuracy))\n",
        "\n",
        "DT_AUC = roc_auc_score(y_valid, DT_Predict)\n",
        "print(\"AUC: \" + str(DT_AUC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcpwoU1Adlbz"
      },
      "source": [
        "**Linear Support Vector Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "REJR7U3Ddlbz"
      },
      "outputs": [],
      "source": [
        "training_LSVC = training\n",
        "X_train_LSVC = X_train\n",
        "y_train_LSVC = y_train\n",
        "X_test_LSVC = songData.drop(training_LSVC.index)[features]\n",
        "X_train_LSVC, X_valid_LSVC, y_train_LSVC, y_valid_LSVC = train_test_split(\n",
        "    X_train_LSVC, y_train_LSVC, test_size = 0.2, random_state = 420)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6trNg2b4dlbz",
        "outputId": "9b794c05-afd6-4099-cd4a-2cee41e0a2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6882242771860002\n",
            "AUC: 0.6015530956570263\n"
          ]
        }
      ],
      "source": [
        "LSVC_Model = DecisionTreeClassifier()\n",
        "LSVC_Model.fit(X_train_LSVC, y_train_LSVC)\n",
        "LSVC_Predict = LSVC_Model.predict(X_valid_LSVC)\n",
        "LSVC_Accuracy = accuracy_score(y_valid_LSVC, LSVC_Predict)\n",
        "print(\"Accuracy: \" + str(LSVC_Accuracy))\n",
        "\n",
        "LSVC_AUC = roc_auc_score(y_valid_LSVC, LSVC_Predict)\n",
        "print(\"AUC: \" + str(LSVC_AUC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEHXMhrzdlb1"
      },
      "source": [
        "**XGBOOST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "powDu0x4dlb1",
        "outputId": "71cef587-05af-4454-e234-0b72f887bb86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7798483004026594\n",
            "AUC: 0.624579080843694\n"
          ]
        }
      ],
      "source": [
        "XGB_Model = XGBClassifier(objective = \"binary:logistic\", n_estimators = 10)\n",
        "XGB_Model.fit(X_train, y_train)\n",
        "XGB_Predict = XGB_Model.predict(X_valid)\n",
        "XGB_Accuracy = accuracy_score(y_valid, XGB_Predict)\n",
        "print(\"Accuracy: \" + str(XGB_Accuracy))\n",
        "\n",
        "XGB_AUC = roc_auc_score(y_valid, XGB_Predict)\n",
        "print(\"AUC: \" + str(XGB_AUC))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LightGBM**"
      ],
      "metadata": {
        "id": "hQ9pGXKPfCxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm = LGBMClassifier(n_estimators=100, learning_rate=0.05)\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lgbm = lgbm.predict(X_valid)\n",
        "y_proba_lgbm = lgbm.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "accuracy_lgbm = accuracy_score(y_valid, y_pred_lgbm)\n",
        "auc_lgbm = roc_auc_score(y_valid, y_proba_lgbm)\n",
        "\n",
        "print(\"LightGBM Accuracy:\", accuracy_lgbm)\n",
        "print(\"LightGBM AUC:\", auc_lgbm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuIwId3Ve6R8",
        "outputId": "d9d44ff1-489e-46f1-d3fe-af5787cbfe78"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 22216, number of negative: 63213\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003721 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 85429, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260052 -> initscore=-1.045697\n",
            "[LightGBM] [Info] Start training from score -1.045697\n",
            "LightGBM Accuracy: 0.7856072665979961\n",
            "LightGBM AUC: 0.7829418007417945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparemeter Tuning**"
      ],
      "metadata": {
        "id": "t6r3tiJIfU9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'num_leaves': [31, 61, 91],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "lgbm = LGBMClassifier()\n",
        "\n",
        "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=3, scoring='roc_auc', verbose=1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best AUC found: \", grid_search.best_score_)\n",
        "\n",
        "y_pred_lgbm = grid_search.best_estimator_.predict(X_valid)\n",
        "y_proba_lgbm = grid_search.best_estimator_.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "accuracy_lgbm = accuracy_score(y_valid, y_pred_lgbm)\n",
        "auc_lgbm = roc_auc_score(y_valid, y_proba_lgbm)\n",
        "\n",
        "print(\"Optimized LightGBM Accuracy:\", accuracy_lgbm)\n",
        "print(\"Optimized LightGBM AUC:\", auc_lgbm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7rze7X5fNY_",
        "outputId": "e6cd3b17-de3d-4ae7-9046-1a8553c12da3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002064 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001948 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030822 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024871 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011190 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036632 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013108 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001957 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007540 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002037 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007161 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013124 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002075 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003678 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002094 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002031 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007406 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001939 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002027 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003788 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002014 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001934 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001933 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002050 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003525 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002032 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003661 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001954 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002021 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001953 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001987 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013929 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002292 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012406 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002058 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001937 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002013 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001929 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003650 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001979 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001966 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002045 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014229 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001925 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002255 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002105 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001980 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002018 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001923 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001926 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002210 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002046 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001946 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002063 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002052 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011625 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002028 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002029 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001928 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002031 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001988 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012635 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002031 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001945 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001938 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007344 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007428 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012168 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002070 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002010 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008412 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002145 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002028 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003373 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003410 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001966 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011967 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012399 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001944 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001953 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001958 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001938 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002039 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003671 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001942 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002079 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002086 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007397 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001941 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011992 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001943 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001963 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007724 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002006 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003638 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013079 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002192 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002315 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001972 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002104 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001927 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002018 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002000 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001988 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001917 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001931 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002091 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012235 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007240 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002021 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001928 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002075 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008448 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012986 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002045 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001967 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001904 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002142 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002034 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002025 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001942 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002025 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002040 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002223 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001937 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003393 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002021 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007222 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002028 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009606 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002071 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002049 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002025 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011715 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001993 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001955 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002074 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002042 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001930 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011648 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001930 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002026 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002075 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001964 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001998 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011254 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012083 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002032 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001943 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002142 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002037 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001941 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002175 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001937 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002017 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001941 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003427 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002482 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001925 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001985 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002008 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011270 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001944 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002041 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008762 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001979 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002017 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010848 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002024 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003770 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010824 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001970 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002072 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001948 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008383 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002127 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002024 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007792 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003798 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012810 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010026 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001959 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016224 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001936 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001961 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011831 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002013 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001943 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001933 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001929 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001935 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001938 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008115 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001945 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002025 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003683 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003385 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012591 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002093 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001925 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002052 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001914 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001954 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002040 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002146 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011048 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002068 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002019 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001958 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001976 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14810, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001988 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260044 -> initscore=-1.045742\n",
            "[LightGBM] [Info] Start training from score -1.045742\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 14811, number of negative: 42142\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001958 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 56953, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260057 -> initscore=-1.045675\n",
            "[LightGBM] [Info] Start training from score -1.045675\n",
            "[LightGBM] [Info] Number of positive: 22216, number of negative: 63213\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010821 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 85429, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260052 -> initscore=-1.045697\n",
            "[LightGBM] [Info] Start training from score -1.045697\n",
            "Best parameters found:  {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 200, 'num_leaves': 61}\n",
            "Best AUC found:  0.7859976176424461\n",
            "Optimized LightGBM Accuracy: 0.7875737428598183\n",
            "Optimized LightGBM AUC: 0.7872493579742914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Voting Ensemble - LGBM, XGB, MLP**"
      ],
      "metadata": {
        "id": "cACmZkeUiv-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "estimators = [\n",
        "    ('lgbm', LGBMClassifier(n_estimators=200, learning_rate=0.05)),\n",
        "    ('xgb', make_pipeline(StandardScaler(), XGBClassifier(use_label_encoder=False, eval_metric='logloss'))),\n",
        "    ('mlp', make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(100,), max_iter=300)))\n",
        "]\n",
        "\n",
        "voting = VotingClassifier(estimators=estimators, voting='soft')\n",
        "\n",
        "voting.fit(X_train, y_train)\n",
        "\n",
        "y_pred_voting = voting.predict(X_valid)\n",
        "y_proba_voting = voting.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "accuracy_voting = accuracy_score(y_valid, y_pred_voting)\n",
        "auc_voting = roc_auc_score(y_valid, y_proba_voting)\n",
        "\n",
        "print(\"Voting Ensemble Accuracy:\", accuracy_voting)\n",
        "print(\"Voting Ensemble AUC:\", auc_voting)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6u6W1p2hlQ4",
        "outputId": "7ec016a3-94e3-4e71-ff1d-e2ec87ff9738"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 22216, number of negative: 63213\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011383 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2306\n",
            "[LightGBM] [Info] Number of data points in the train set: 85429, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260052 -> initscore=-1.045697\n",
            "[LightGBM] [Info] Start training from score -1.045697\n",
            "Voting Ensemble Accuracy: 0.7894465773948872\n",
            "Voting Ensemble AUC: 0.7890296992730962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning - Neural Networks**"
      ],
      "metadata": {
        "id": "BY2qaPfFicEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "num_classes = y_train.nunique()\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_valid_cat = to_categorical(y_valid, num_classes)\n",
        "\n",
        "# Neural network architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')  # Use 'sigmoid' if it's binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Use 'binary_crossentropy' for binary classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train_cat, epochs=50, batch_size=32, validation_data=(X_valid_scaled, y_valid_cat), verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_prob = model.predict(X_valid_scaled)\n",
        "y_pred = y_pred_prob.argmax(axis=1)  # Use (y_pred_prob > 0.5).astype(int) for binary classification\n",
        "\n",
        "# Calculate accuracy and AUC\n",
        "accuracy_dl1 = accuracy_score(y_valid, y_pred)\n",
        "auc_dl1 = roc_auc_score(y_valid_cat, y_pred_prob)  # Ensure y_valid_cat is used for multiclass AUC\n",
        "\n",
        "print(\"Deep Learning Model Accuracy:\", accuracy_dl1)\n",
        "print(\"Deep Learning Model AUC:\", auc_dl1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk-zdsdXiWfM",
        "outputId": "004ed97c-8710-4e7b-b7f1-8ab64c1f077d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2670/2670 [==============================] - 11s 4ms/step - loss: 0.5120 - accuracy: 0.7536 - val_loss: 0.4871 - val_accuracy: 0.7688\n",
            "Epoch 2/50\n",
            "2670/2670 [==============================] - 9s 4ms/step - loss: 0.4967 - accuracy: 0.7633 - val_loss: 0.4839 - val_accuracy: 0.7728\n",
            "Epoch 3/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4924 - accuracy: 0.7654 - val_loss: 0.4806 - val_accuracy: 0.7750\n",
            "Epoch 4/50\n",
            "2670/2670 [==============================] - 9s 3ms/step - loss: 0.4901 - accuracy: 0.7676 - val_loss: 0.4812 - val_accuracy: 0.7731\n",
            "Epoch 5/50\n",
            "2670/2670 [==============================] - 9s 3ms/step - loss: 0.4892 - accuracy: 0.7683 - val_loss: 0.4772 - val_accuracy: 0.7768\n",
            "Epoch 6/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4881 - accuracy: 0.7691 - val_loss: 0.4771 - val_accuracy: 0.7785\n",
            "Epoch 7/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4877 - accuracy: 0.7702 - val_loss: 0.4766 - val_accuracy: 0.7774\n",
            "Epoch 8/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4876 - accuracy: 0.7697 - val_loss: 0.4764 - val_accuracy: 0.7785\n",
            "Epoch 9/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4870 - accuracy: 0.7699 - val_loss: 0.4764 - val_accuracy: 0.7791\n",
            "Epoch 10/50\n",
            "2670/2670 [==============================] - 11s 4ms/step - loss: 0.4860 - accuracy: 0.7693 - val_loss: 0.4753 - val_accuracy: 0.7802\n",
            "Epoch 11/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4860 - accuracy: 0.7697 - val_loss: 0.4767 - val_accuracy: 0.7790\n",
            "Epoch 12/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4859 - accuracy: 0.7688 - val_loss: 0.4773 - val_accuracy: 0.7796\n",
            "Epoch 13/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4865 - accuracy: 0.7690 - val_loss: 0.4738 - val_accuracy: 0.7790\n",
            "Epoch 14/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4855 - accuracy: 0.7701 - val_loss: 0.4760 - val_accuracy: 0.7778\n",
            "Epoch 15/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4849 - accuracy: 0.7699 - val_loss: 0.4758 - val_accuracy: 0.7783\n",
            "Epoch 16/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4845 - accuracy: 0.7704 - val_loss: 0.4776 - val_accuracy: 0.7792\n",
            "Epoch 17/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4844 - accuracy: 0.7703 - val_loss: 0.4730 - val_accuracy: 0.7795\n",
            "Epoch 18/50\n",
            "2670/2670 [==============================] - 9s 3ms/step - loss: 0.4847 - accuracy: 0.7719 - val_loss: 0.4751 - val_accuracy: 0.7800\n",
            "Epoch 19/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4850 - accuracy: 0.7687 - val_loss: 0.4742 - val_accuracy: 0.7800\n",
            "Epoch 20/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4842 - accuracy: 0.7698 - val_loss: 0.4747 - val_accuracy: 0.7788\n",
            "Epoch 21/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4836 - accuracy: 0.7704 - val_loss: 0.4750 - val_accuracy: 0.7789\n",
            "Epoch 22/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4839 - accuracy: 0.7711 - val_loss: 0.4731 - val_accuracy: 0.7796\n",
            "Epoch 23/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4835 - accuracy: 0.7710 - val_loss: 0.4763 - val_accuracy: 0.7806\n",
            "Epoch 24/50\n",
            "2670/2670 [==============================] - 7s 2ms/step - loss: 0.4834 - accuracy: 0.7710 - val_loss: 0.4739 - val_accuracy: 0.7806\n",
            "Epoch 25/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4833 - accuracy: 0.7702 - val_loss: 0.4729 - val_accuracy: 0.7799\n",
            "Epoch 26/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4838 - accuracy: 0.7703 - val_loss: 0.4775 - val_accuracy: 0.7792\n",
            "Epoch 27/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4834 - accuracy: 0.7704 - val_loss: 0.4734 - val_accuracy: 0.7801\n",
            "Epoch 28/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4832 - accuracy: 0.7717 - val_loss: 0.4752 - val_accuracy: 0.7794\n",
            "Epoch 29/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4830 - accuracy: 0.7704 - val_loss: 0.4742 - val_accuracy: 0.7798\n",
            "Epoch 30/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4835 - accuracy: 0.7709 - val_loss: 0.4736 - val_accuracy: 0.7798\n",
            "Epoch 31/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4833 - accuracy: 0.7708 - val_loss: 0.4729 - val_accuracy: 0.7802\n",
            "Epoch 32/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4828 - accuracy: 0.7720 - val_loss: 0.4737 - val_accuracy: 0.7790\n",
            "Epoch 33/50\n",
            "2670/2670 [==============================] - 9s 3ms/step - loss: 0.4839 - accuracy: 0.7698 - val_loss: 0.4753 - val_accuracy: 0.7798\n",
            "Epoch 34/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4831 - accuracy: 0.7707 - val_loss: 0.4752 - val_accuracy: 0.7797\n",
            "Epoch 35/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4832 - accuracy: 0.7719 - val_loss: 0.4744 - val_accuracy: 0.7780\n",
            "Epoch 36/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4838 - accuracy: 0.7700 - val_loss: 0.4752 - val_accuracy: 0.7792\n",
            "Epoch 37/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4823 - accuracy: 0.7721 - val_loss: 0.4726 - val_accuracy: 0.7806\n",
            "Epoch 38/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4820 - accuracy: 0.7725 - val_loss: 0.4739 - val_accuracy: 0.7788\n",
            "Epoch 39/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4821 - accuracy: 0.7722 - val_loss: 0.4736 - val_accuracy: 0.7798\n",
            "Epoch 40/50\n",
            "2670/2670 [==============================] - 9s 3ms/step - loss: 0.4840 - accuracy: 0.7702 - val_loss: 0.4735 - val_accuracy: 0.7785\n",
            "Epoch 41/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4825 - accuracy: 0.7717 - val_loss: 0.4738 - val_accuracy: 0.7798\n",
            "Epoch 42/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4836 - accuracy: 0.7707 - val_loss: 0.4732 - val_accuracy: 0.7785\n",
            "Epoch 43/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4827 - accuracy: 0.7712 - val_loss: 0.4736 - val_accuracy: 0.7780\n",
            "Epoch 44/50\n",
            "2670/2670 [==============================] - 7s 2ms/step - loss: 0.4816 - accuracy: 0.7713 - val_loss: 0.4726 - val_accuracy: 0.7786\n",
            "Epoch 45/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4827 - accuracy: 0.7701 - val_loss: 0.4723 - val_accuracy: 0.7790\n",
            "Epoch 46/50\n",
            "2670/2670 [==============================] - 9s 3ms/step - loss: 0.4829 - accuracy: 0.7708 - val_loss: 0.4733 - val_accuracy: 0.7787\n",
            "Epoch 47/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4828 - accuracy: 0.7703 - val_loss: 0.4735 - val_accuracy: 0.7787\n",
            "Epoch 48/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4826 - accuracy: 0.7707 - val_loss: 0.4744 - val_accuracy: 0.7791\n",
            "Epoch 49/50\n",
            "2670/2670 [==============================] - 7s 3ms/step - loss: 0.4833 - accuracy: 0.7715 - val_loss: 0.4770 - val_accuracy: 0.7776\n",
            "Epoch 50/50\n",
            "2670/2670 [==============================] - 8s 3ms/step - loss: 0.4830 - accuracy: 0.7716 - val_loss: 0.4732 - val_accuracy: 0.7785\n",
            "668/668 [==============================] - 1s 1ms/step\n",
            "Deep Learning Model Accuracy: 0.7784904953647346\n",
            "Deep Learning Model AUC: 0.7766162324259065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deeper Neural Network**"
      ],
      "metadata": {
        "id": "g7fIOG-skbhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "# Convert labels to categorical\n",
        "num_classes = y_train.nunique()\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_valid_cat = to_categorical(y_valid, num_classes)\n",
        "\n",
        "# More complex neural network architecture\n",
        "model = Sequential([\n",
        "    Dense(256, input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(128),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(num_classes, activation='softmax')  # Use 'sigmoid' if it's binary classification and change last layer to Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Change to 'binary_crossentropy' for binary classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train_cat, epochs=100, batch_size=32, validation_data=(X_valid_scaled, y_valid_cat), verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_prob = model.predict(X_valid_scaled)\n",
        "y_pred = y_pred_prob.argmax(axis=1)  # Use (y_pred_prob > 0.5).astype(int) for binary classification\n",
        "\n",
        "# Calculate accuracy and AUC\n",
        "accuracy_dl2 = accuracy_score(y_valid, y_pred)\n",
        "auc_dl2 = roc_auc_score(y_valid_cat, y_pred_prob)  # Ensure y_valid_cat is used for multiclass AUC\n",
        "\n",
        "print(\"Deep Learning Model Accuracy:\", accuracy_dl2)\n",
        "print(\"Deep Learning Model AUC:\", auc_dl2)"
      ],
      "metadata": {
        "id": "HrpC9rGOkgzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7608496-dc7c-4b72-9351-cf5da1ebe494"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2670/2670 [==============================] - 16s 5ms/step - loss: 0.5192 - accuracy: 0.7508 - val_loss: 0.4849 - val_accuracy: 0.7716\n",
            "Epoch 2/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4966 - accuracy: 0.7618 - val_loss: 0.4797 - val_accuracy: 0.7743\n",
            "Epoch 3/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4935 - accuracy: 0.7637 - val_loss: 0.4826 - val_accuracy: 0.7757\n",
            "Epoch 4/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4915 - accuracy: 0.7647 - val_loss: 0.4798 - val_accuracy: 0.7751\n",
            "Epoch 5/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4897 - accuracy: 0.7666 - val_loss: 0.4779 - val_accuracy: 0.7772\n",
            "Epoch 6/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4884 - accuracy: 0.7670 - val_loss: 0.4767 - val_accuracy: 0.7783\n",
            "Epoch 7/100\n",
            "2670/2670 [==============================] - 12s 5ms/step - loss: 0.4883 - accuracy: 0.7671 - val_loss: 0.4752 - val_accuracy: 0.7770\n",
            "Epoch 8/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4865 - accuracy: 0.7689 - val_loss: 0.4741 - val_accuracy: 0.7779\n",
            "Epoch 9/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4863 - accuracy: 0.7681 - val_loss: 0.4732 - val_accuracy: 0.7778\n",
            "Epoch 10/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4864 - accuracy: 0.7686 - val_loss: 0.4728 - val_accuracy: 0.7787\n",
            "Epoch 11/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4856 - accuracy: 0.7699 - val_loss: 0.4718 - val_accuracy: 0.7797\n",
            "Epoch 12/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4861 - accuracy: 0.7685 - val_loss: 0.4737 - val_accuracy: 0.7786\n",
            "Epoch 13/100\n",
            "2670/2670 [==============================] - 12s 5ms/step - loss: 0.4859 - accuracy: 0.7688 - val_loss: 0.4735 - val_accuracy: 0.7799\n",
            "Epoch 14/100\n",
            "2670/2670 [==============================] - 15s 6ms/step - loss: 0.4839 - accuracy: 0.7694 - val_loss: 0.4712 - val_accuracy: 0.7793\n",
            "Epoch 15/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4841 - accuracy: 0.7707 - val_loss: 0.4725 - val_accuracy: 0.7801\n",
            "Epoch 16/100\n",
            "2670/2670 [==============================] - 12s 5ms/step - loss: 0.4828 - accuracy: 0.7721 - val_loss: 0.4716 - val_accuracy: 0.7813\n",
            "Epoch 17/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4842 - accuracy: 0.7705 - val_loss: 0.4704 - val_accuracy: 0.7799\n",
            "Epoch 18/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4839 - accuracy: 0.7696 - val_loss: 0.4706 - val_accuracy: 0.7823\n",
            "Epoch 19/100\n",
            "2670/2670 [==============================] - 12s 5ms/step - loss: 0.4829 - accuracy: 0.7703 - val_loss: 0.4740 - val_accuracy: 0.7778\n",
            "Epoch 20/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4820 - accuracy: 0.7706 - val_loss: 0.4723 - val_accuracy: 0.7811\n",
            "Epoch 21/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4823 - accuracy: 0.7700 - val_loss: 0.4717 - val_accuracy: 0.7806\n",
            "Epoch 22/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4821 - accuracy: 0.7711 - val_loss: 0.4685 - val_accuracy: 0.7818\n",
            "Epoch 23/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4821 - accuracy: 0.7707 - val_loss: 0.4702 - val_accuracy: 0.7817\n",
            "Epoch 24/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4818 - accuracy: 0.7697 - val_loss: 0.4684 - val_accuracy: 0.7830\n",
            "Epoch 25/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4806 - accuracy: 0.7727 - val_loss: 0.4698 - val_accuracy: 0.7807\n",
            "Epoch 26/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4806 - accuracy: 0.7720 - val_loss: 0.4699 - val_accuracy: 0.7806\n",
            "Epoch 27/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4819 - accuracy: 0.7709 - val_loss: 0.4691 - val_accuracy: 0.7821\n",
            "Epoch 28/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4809 - accuracy: 0.7719 - val_loss: 0.4698 - val_accuracy: 0.7805\n",
            "Epoch 29/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4813 - accuracy: 0.7722 - val_loss: 0.4695 - val_accuracy: 0.7797\n",
            "Epoch 30/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4801 - accuracy: 0.7726 - val_loss: 0.4692 - val_accuracy: 0.7801\n",
            "Epoch 31/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4807 - accuracy: 0.7718 - val_loss: 0.4715 - val_accuracy: 0.7791\n",
            "Epoch 32/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4800 - accuracy: 0.7724 - val_loss: 0.4687 - val_accuracy: 0.7801\n",
            "Epoch 33/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4816 - accuracy: 0.7712 - val_loss: 0.4686 - val_accuracy: 0.7818\n",
            "Epoch 34/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4807 - accuracy: 0.7722 - val_loss: 0.4682 - val_accuracy: 0.7818\n",
            "Epoch 35/100\n",
            "2670/2670 [==============================] - 15s 6ms/step - loss: 0.4794 - accuracy: 0.7715 - val_loss: 0.4696 - val_accuracy: 0.7798\n",
            "Epoch 36/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4809 - accuracy: 0.7716 - val_loss: 0.4682 - val_accuracy: 0.7819\n",
            "Epoch 37/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4808 - accuracy: 0.7713 - val_loss: 0.4716 - val_accuracy: 0.7833\n",
            "Epoch 38/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4790 - accuracy: 0.7731 - val_loss: 0.4679 - val_accuracy: 0.7824\n",
            "Epoch 39/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4783 - accuracy: 0.7720 - val_loss: 0.4704 - val_accuracy: 0.7811\n",
            "Epoch 40/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4796 - accuracy: 0.7727 - val_loss: 0.4697 - val_accuracy: 0.7824\n",
            "Epoch 41/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4799 - accuracy: 0.7715 - val_loss: 0.4678 - val_accuracy: 0.7818\n",
            "Epoch 42/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4792 - accuracy: 0.7722 - val_loss: 0.4674 - val_accuracy: 0.7825\n",
            "Epoch 43/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4785 - accuracy: 0.7732 - val_loss: 0.4687 - val_accuracy: 0.7809\n",
            "Epoch 44/100\n",
            "2670/2670 [==============================] - 15s 6ms/step - loss: 0.4798 - accuracy: 0.7728 - val_loss: 0.4680 - val_accuracy: 0.7813\n",
            "Epoch 45/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4792 - accuracy: 0.7726 - val_loss: 0.4688 - val_accuracy: 0.7821\n",
            "Epoch 46/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4788 - accuracy: 0.7725 - val_loss: 0.4689 - val_accuracy: 0.7797\n",
            "Epoch 47/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4789 - accuracy: 0.7720 - val_loss: 0.4680 - val_accuracy: 0.7824\n",
            "Epoch 48/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4791 - accuracy: 0.7725 - val_loss: 0.4685 - val_accuracy: 0.7819\n",
            "Epoch 49/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4797 - accuracy: 0.7720 - val_loss: 0.4684 - val_accuracy: 0.7821\n",
            "Epoch 50/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4788 - accuracy: 0.7715 - val_loss: 0.4684 - val_accuracy: 0.7806\n",
            "Epoch 51/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4782 - accuracy: 0.7724 - val_loss: 0.4684 - val_accuracy: 0.7817\n",
            "Epoch 52/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4785 - accuracy: 0.7729 - val_loss: 0.4708 - val_accuracy: 0.7828\n",
            "Epoch 53/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4779 - accuracy: 0.7741 - val_loss: 0.4683 - val_accuracy: 0.7805\n",
            "Epoch 54/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4787 - accuracy: 0.7724 - val_loss: 0.4685 - val_accuracy: 0.7817\n",
            "Epoch 55/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4792 - accuracy: 0.7727 - val_loss: 0.4679 - val_accuracy: 0.7831\n",
            "Epoch 56/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4789 - accuracy: 0.7726 - val_loss: 0.4671 - val_accuracy: 0.7824\n",
            "Epoch 57/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4790 - accuracy: 0.7729 - val_loss: 0.4684 - val_accuracy: 0.7818\n",
            "Epoch 58/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4781 - accuracy: 0.7719 - val_loss: 0.4684 - val_accuracy: 0.7821\n",
            "Epoch 59/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4778 - accuracy: 0.7732 - val_loss: 0.4678 - val_accuracy: 0.7835\n",
            "Epoch 60/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4778 - accuracy: 0.7742 - val_loss: 0.4678 - val_accuracy: 0.7812\n",
            "Epoch 61/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4779 - accuracy: 0.7730 - val_loss: 0.4684 - val_accuracy: 0.7809\n",
            "Epoch 62/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4788 - accuracy: 0.7728 - val_loss: 0.4680 - val_accuracy: 0.7825\n",
            "Epoch 63/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4771 - accuracy: 0.7736 - val_loss: 0.4678 - val_accuracy: 0.7827\n",
            "Epoch 64/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4781 - accuracy: 0.7739 - val_loss: 0.4671 - val_accuracy: 0.7811\n",
            "Epoch 65/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4790 - accuracy: 0.7728 - val_loss: 0.4680 - val_accuracy: 0.7812\n",
            "Epoch 66/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4774 - accuracy: 0.7741 - val_loss: 0.4690 - val_accuracy: 0.7828\n",
            "Epoch 67/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4774 - accuracy: 0.7734 - val_loss: 0.4676 - val_accuracy: 0.7836\n",
            "Epoch 68/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4780 - accuracy: 0.7731 - val_loss: 0.4673 - val_accuracy: 0.7827\n",
            "Epoch 69/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4772 - accuracy: 0.7742 - val_loss: 0.4689 - val_accuracy: 0.7843\n",
            "Epoch 70/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4774 - accuracy: 0.7734 - val_loss: 0.4669 - val_accuracy: 0.7820\n",
            "Epoch 71/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4770 - accuracy: 0.7743 - val_loss: 0.4676 - val_accuracy: 0.7820\n",
            "Epoch 72/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4774 - accuracy: 0.7735 - val_loss: 0.4681 - val_accuracy: 0.7821\n",
            "Epoch 73/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4781 - accuracy: 0.7738 - val_loss: 0.4680 - val_accuracy: 0.7835\n",
            "Epoch 74/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4778 - accuracy: 0.7739 - val_loss: 0.4673 - val_accuracy: 0.7827\n",
            "Epoch 75/100\n",
            "2670/2670 [==============================] - 15s 5ms/step - loss: 0.4772 - accuracy: 0.7742 - val_loss: 0.4692 - val_accuracy: 0.7828\n",
            "Epoch 76/100\n",
            "2670/2670 [==============================] - 15s 5ms/step - loss: 0.4770 - accuracy: 0.7740 - val_loss: 0.4679 - val_accuracy: 0.7827\n",
            "Epoch 77/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4766 - accuracy: 0.7738 - val_loss: 0.4675 - val_accuracy: 0.7829\n",
            "Epoch 78/100\n",
            "2670/2670 [==============================] - 15s 5ms/step - loss: 0.4765 - accuracy: 0.7738 - val_loss: 0.4679 - val_accuracy: 0.7831\n",
            "Epoch 79/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4770 - accuracy: 0.7734 - val_loss: 0.4667 - val_accuracy: 0.7835\n",
            "Epoch 80/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4762 - accuracy: 0.7747 - val_loss: 0.4675 - val_accuracy: 0.7816\n",
            "Epoch 81/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4768 - accuracy: 0.7745 - val_loss: 0.4667 - val_accuracy: 0.7822\n",
            "Epoch 82/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4769 - accuracy: 0.7733 - val_loss: 0.4674 - val_accuracy: 0.7833\n",
            "Epoch 83/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4761 - accuracy: 0.7748 - val_loss: 0.4669 - val_accuracy: 0.7827\n",
            "Epoch 84/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4771 - accuracy: 0.7733 - val_loss: 0.4669 - val_accuracy: 0.7824\n",
            "Epoch 85/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4765 - accuracy: 0.7741 - val_loss: 0.4681 - val_accuracy: 0.7832\n",
            "Epoch 86/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4764 - accuracy: 0.7741 - val_loss: 0.4671 - val_accuracy: 0.7825\n",
            "Epoch 87/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4763 - accuracy: 0.7738 - val_loss: 0.4671 - val_accuracy: 0.7845\n",
            "Epoch 88/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4763 - accuracy: 0.7743 - val_loss: 0.4672 - val_accuracy: 0.7831\n",
            "Epoch 89/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4768 - accuracy: 0.7747 - val_loss: 0.4697 - val_accuracy: 0.7826\n",
            "Epoch 90/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4773 - accuracy: 0.7740 - val_loss: 0.4671 - val_accuracy: 0.7829\n",
            "Epoch 91/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4764 - accuracy: 0.7740 - val_loss: 0.4681 - val_accuracy: 0.7818\n",
            "Epoch 92/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4758 - accuracy: 0.7748 - val_loss: 0.4667 - val_accuracy: 0.7819\n",
            "Epoch 93/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4769 - accuracy: 0.7749 - val_loss: 0.4676 - val_accuracy: 0.7812\n",
            "Epoch 94/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4760 - accuracy: 0.7749 - val_loss: 0.4672 - val_accuracy: 0.7837\n",
            "Epoch 95/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4764 - accuracy: 0.7745 - val_loss: 0.4671 - val_accuracy: 0.7822\n",
            "Epoch 96/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4768 - accuracy: 0.7740 - val_loss: 0.4667 - val_accuracy: 0.7844\n",
            "Epoch 97/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4756 - accuracy: 0.7736 - val_loss: 0.4663 - val_accuracy: 0.7827\n",
            "Epoch 98/100\n",
            "2670/2670 [==============================] - 14s 5ms/step - loss: 0.4760 - accuracy: 0.7755 - val_loss: 0.4678 - val_accuracy: 0.7825\n",
            "Epoch 99/100\n",
            "2670/2670 [==============================] - 15s 6ms/step - loss: 0.4762 - accuracy: 0.7737 - val_loss: 0.4669 - val_accuracy: 0.7833\n",
            "Epoch 100/100\n",
            "2670/2670 [==============================] - 13s 5ms/step - loss: 0.4755 - accuracy: 0.7752 - val_loss: 0.4669 - val_accuracy: 0.7830\n",
            "668/668 [==============================] - 2s 3ms/step\n",
            "Deep Learning Model Accuracy: 0.7829852982488997\n",
            "Deep Learning Model AUC: 0.7827392676897842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Htnbh9zkdzLG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW2Mms5sdlb1"
      },
      "source": [
        "**Model Performance Summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "PIFrxq2gdlb1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "model_performance_accuracy = pd.DataFrame({'Model': ['LogisticRegression',\n",
        "                                                      'RandomForestClassifier',\n",
        "                                                      'KNeighborsClassifier',\n",
        "                                                      'DecisionTreeClassifier',\n",
        "                                                      'LinearSVC',\n",
        "                                                      'XGBClassifier',\n",
        "                                                      'LGBMClassifier',\n",
        "                                                      'Voting_Ensemble',\n",
        "                                                      'NeuralNetwork1',\n",
        "                                                      'NeuralNetwork2'\n",
        "                                                     ],\n",
        "                                            'Accuracy': [LR_Accuracy,\n",
        "                                                         RFC_Accuracy,\n",
        "                                                         KNN_Accuracy,\n",
        "                                                         DT_Accuracy,\n",
        "                                                         LSVC_Accuracy,\n",
        "                                                         XGB_Accuracy,\n",
        "                                                         accuracy_lgbm,\n",
        "                                                         accuracy_voting,\n",
        "                                                         accuracy_dl1,\n",
        "                                                         accuracy_dl2\n",
        "                                                         ]})\n",
        "\n",
        "model_performance_AUC = pd.DataFrame({'Model': ['LogisticRegression',\n",
        "                                                 'RandomForestClassifier',\n",
        "                                                 'KNeighborsClassifier',\n",
        "                                                 'DecisionTreeClassifier',\n",
        "                                                 'LinearSVC',\n",
        "                                                 'XGBClassifier',\n",
        "                                                 'LGBMClassifier',\n",
        "                                                 'Voting_Ensemble',\n",
        "                                                 'NeuralNetwork1',\n",
        "                                                 'NeuralNetwork2'\n",
        "                                                ],\n",
        "                                      'AUC': [LR_AUC,\n",
        "                                              RFC_AUC,\n",
        "                                              KNN_AUC,\n",
        "                                              DT_AUC,\n",
        "                                              LSVC_AUC,\n",
        "                                              XGB_AUC,\n",
        "                                              auc_lgbm,\n",
        "                                              auc_voting,\n",
        "                                              auc_dl1,\n",
        "                                              auc_dl2\n",
        "                                             ]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3ZDpE9z8dlb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "54548e63-54ea-4f83-b261-400218492232"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Model  Accuracy\n",
              "7         Voting_Ensemble  0.789447\n",
              "1  RandomForestClassifier  0.787574\n",
              "6          LGBMClassifier  0.787574\n",
              "9          NeuralNetwork2  0.782985\n",
              "5           XGBClassifier  0.779848\n",
              "8          NeuralNetwork1  0.778490\n",
              "0      LogisticRegression  0.742111\n",
              "2    KNeighborsClassifier  0.695898\n",
              "3  DecisionTreeClassifier  0.694072\n",
              "4               LinearSVC  0.688224"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff1288ec-1e46-4a81-8a04-fef4e1f5b31f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Voting_Ensemble</td>\n",
              "      <td>0.789447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.787574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LGBMClassifier</td>\n",
              "      <td>0.787574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NeuralNetwork2</td>\n",
              "      <td>0.782985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBClassifier</td>\n",
              "      <td>0.779848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NeuralNetwork1</td>\n",
              "      <td>0.778490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.742111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KNeighborsClassifier</td>\n",
              "      <td>0.695898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "      <td>0.694072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>0.688224</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff1288ec-1e46-4a81-8a04-fef4e1f5b31f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ff1288ec-1e46-4a81-8a04-fef4e1f5b31f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ff1288ec-1e46-4a81-8a04-fef4e1f5b31f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-df68de43-fe3b-4861-baf3-60ffbf3fe7c7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df68de43-fe3b-4861-baf3-60ffbf3fe7c7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-df68de43-fe3b-4861-baf3-60ffbf3fe7c7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"model_performance_accuracy\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"DecisionTreeClassifier\",\n          \"RandomForestClassifier\",\n          \"NeuralNetwork1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.043505856237689135,\n        \"min\": 0.6882242771860002,\n        \"max\": 0.7894465773948872,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.6940724786965071,\n          0.7875737428598183,\n          0.7421106845210226\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "model_performance_accuracy.sort_values(by = \"Accuracy\", ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "KdPVwn8edlb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "18ce5a6a-bcbb-421d-eed4-b4f5fbff1bec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Model       AUC\n",
              "7         Voting_Ensemble  0.789030\n",
              "6          LGBMClassifier  0.787249\n",
              "9          NeuralNetwork2  0.782739\n",
              "8          NeuralNetwork1  0.776616\n",
              "1  RandomForestClassifier  0.647376\n",
              "5           XGBClassifier  0.624579\n",
              "3  DecisionTreeClassifier  0.609552\n",
              "4               LinearSVC  0.601553\n",
              "2    KNeighborsClassifier  0.530702\n",
              "0      LogisticRegression  0.500000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f549370f-66ef-4e03-b967-bf6264476cae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Voting_Ensemble</td>\n",
              "      <td>0.789030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LGBMClassifier</td>\n",
              "      <td>0.787249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NeuralNetwork2</td>\n",
              "      <td>0.782739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NeuralNetwork1</td>\n",
              "      <td>0.776616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.647376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBClassifier</td>\n",
              "      <td>0.624579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "      <td>0.609552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>0.601553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KNeighborsClassifier</td>\n",
              "      <td>0.530702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f549370f-66ef-4e03-b967-bf6264476cae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f549370f-66ef-4e03-b967-bf6264476cae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f549370f-66ef-4e03-b967-bf6264476cae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f51a3456-b536-45c2-ab8b-a582560231d8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f51a3456-b536-45c2-ab8b-a582560231d8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f51a3456-b536-45c2-ab8b-a582560231d8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"model_performance_AUC\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"KNeighborsClassifier\",\n          \"LGBMClassifier\",\n          \"XGBClassifier\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11103271285122873,\n        \"min\": 0.5,\n        \"max\": 0.7890296992730962,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.5307018411991505,\n          0.7872493579742914,\n          0.624579080843694\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "model_performance_AUC.sort_values(by = \"AUC\", ascending = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovSXzBSPdlb2"
      },
      "source": [
        "## Results\n",
        "We provide a summary table with the accuracy and AUC values for every model.\n",
        "\n",
        "At an individual level the Random Forest Classifier and XGBoost algorithms perform the best in terms of accuracy and AUC, with the RandomForestClassifier algorithm achieving the highest accuracy of 0.783734 and AUC of 0.644808.\n",
        "\n",
        "The reason for Voting Ensemble being the highest is because it is a collective model of LGBM, XGB, MLP having an accuracy of 0.789447 and AUC of 0.789030."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw95eE2ndlb2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx5Lhtv5dlb2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbNF1P60dlb2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}